{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4581df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "          Loading Config and Recreating Framework           \n",
      "============================================================\n",
      "Loading dataset via the FlowTransformer framework...\n",
      "Using cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_A6N00gtYIhwW1x05bzV0RseOHrU0.feather\n",
      "Reading directly from cache cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_A6N00gtYIhwW1x05bzV0RseOHrU0.feather...\n",
      "Dataset loaded and processed by the framework.\n",
      "\n",
      "============================================================\n",
      "           Building Model with Corrected Classes            \n",
      "============================================================\n",
      "Model built successfully!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_NUM_PKTS_UP_TO_128_BYTES  [(None, 8, 1)]      0           []                               \n",
      "  (InputLayer)                                                                                    \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_AVG_THROUGHPU  [(None, 8, 1)]      0           []                               \n",
      " T (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_DST_TO_SRC_SECOND_BYTES   [(None, 8, 1)]      0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_IN_BYTES (InputLayer)    [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_FLOW_DURATION_MILLISECON  [(None, 8, 1)]      0           []                               \n",
      " DS (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_1024_TO_1514_BY  [(None, 8, 1)]      0           []                               \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_256_TO_512_BYTE  [(None, 8, 1)]      0           []                               \n",
      " S (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_IN_PKTS (InputLayer)     [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_PKTS (I  [(None, 8, 1)]      0           []                               \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_DST_TO_SRC_AVG_THROUGHPU  [(None, 8, 1)]      0           []                               \n",
      " T (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_IN (InputLay  [(None, 8, 1)]      0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_MAX_TTL (InputLayer)     [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_DURATION_OUT (InputLayer  [(None, 8, 1)]      0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_PKTS (  [(None, 8, 1)]      0           []                               \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_OUT (InputLa  [(None, 8, 1)]      0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_BYTES (  [(None, 8, 1)]      0           []                               \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_LONGEST_FLOW_PKT (InputL  [(None, 8, 1)]      0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_128_TO_256_BYTE  [(None, 8, 1)]      0           []                               \n",
      " S (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_OUT_PKTS (InputLayer)    [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_512_TO_1024_BYT  [(None, 8, 1)]      0           []                               \n",
      " ES (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_MAX_IP_PKT_LEN (InputLay  [(None, 8, 1)]      0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_BYTES   [(None, 8, 1)]      0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_OUT_BYTES (InputLayer)   [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_DURATION_IN (InputLayer)  [(None, 8, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " input_MIN_TTL (InputLayer)     [(None, 8, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_SHORTEST_FLOW_PKT (Input  [(None, 8, 1)]      0           []                               \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_SECOND_BYTES   [(None, 8, 1)]      0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_MIN_IP_PKT_LEN (InputLay  [(None, 8, 1)]      0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_CLIENT_TCP_FLAGS (InputL  [(None, 8, 32)]     0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_L4_SRC_PORT (InputLayer)  [(None, 8, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " input_TCP_FLAGS (InputLayer)   [(None, 8, 32)]      0           []                               \n",
      "                                                                                                  \n",
      " input_ICMP_IPV4_TYPE (InputLay  [(None, 8, 32)]     0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ICMP_TYPE (InputLayer)   [(None, 8, 32)]      0           []                               \n",
      "                                                                                                  \n",
      " input_PROTOCOL (InputLayer)    [(None, 8, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " input_SERVER_TCP_FLAGS (InputL  [(None, 8, 32)]     0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_L4_DST_PORT (InputLayer)  [(None, 8, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " input_L7_PROTO (InputLayer)    [(None, 8, 32)]      0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 289)       0           ['input_NUM_PKTS_UP_TO_128_BYTES[\n",
      "                                                                 0][0]',                          \n",
      "                                                                  'input_SRC_TO_DST_AVG_THROUGHPUT\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'input_DST_TO_SRC_SECOND_BYTES[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'input_IN_BYTES[0][0]',         \n",
      "                                                                  'input_FLOW_DURATION_MILLISECOND\n",
      "                                                                 S[0][0]',                        \n",
      "                                                                  'input_NUM_PKTS_1024_TO_1514_BYT\n",
      "                                                                 ES[0][0]',                       \n",
      "                                                                  'input_NUM_PKTS_256_TO_512_BYTES\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'input_IN_PKTS[0][0]',          \n",
      "                                                                  'input_RETRANSMITTED_IN_PKTS[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'input_DST_TO_SRC_AVG_THROUGHPUT\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'input_TCP_WIN_MAX_IN[0][0]',   \n",
      "                                                                  'input_MAX_TTL[0][0]',          \n",
      "                                                                  'input_DURATION_OUT[0][0]',     \n",
      "                                                                  'input_RETRANSMITTED_OUT_PKTS[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'input_TCP_WIN_MAX_OUT[0][0]',  \n",
      "                                                                  'input_RETRANSMITTED_IN_BYTES[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'input_LONGEST_FLOW_PKT[0][0]', \n",
      "                                                                  'input_NUM_PKTS_128_TO_256_BYTES\n",
      "                                                                 [0][0]',                         \n",
      "                                                                  'input_OUT_PKTS[0][0]',         \n",
      "                                                                  'input_NUM_PKTS_512_TO_1024_BYTE\n",
      "                                                                 S[0][0]',                        \n",
      "                                                                  'input_MAX_IP_PKT_LEN[0][0]',   \n",
      "                                                                  'input_RETRANSMITTED_OUT_BYTES[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'input_OUT_BYTES[0][0]',        \n",
      "                                                                  'input_DURATION_IN[0][0]',      \n",
      "                                                                  'input_MIN_TTL[0][0]',          \n",
      "                                                                  'input_SHORTEST_FLOW_PKT[0][0]',\n",
      "                                                                  'input_SRC_TO_DST_SECOND_BYTES[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'input_MIN_IP_PKT_LEN[0][0]',   \n",
      "                                                                  'input_CLIENT_TCP_FLAGS[0][0]', \n",
      "                                                                  'input_L4_SRC_PORT[0][0]',      \n",
      "                                                                  'input_TCP_FLAGS[0][0]',        \n",
      "                                                                  'input_ICMP_IPV4_TYPE[0][0]',   \n",
      "                                                                  'input_ICMP_TYPE[0][0]',        \n",
      "                                                                  'input_PROTOCOL[0][0]',         \n",
      "                                                                  'input_SERVER_TCP_FLAGS[0][0]', \n",
      "                                                                  'input_L4_DST_PORT[0][0]',      \n",
      "                                                                  'input_L7_PROTO[0][0]']         \n",
      "                                                                                                  \n",
      " block_0_ (TransformerEncoderBl  (None, 8, 289)      372550      ['concatenate_1[0][0]']          \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " block_1_ (TransformerEncoderBl  (None, 8, 289)      372550      ['block_0_[0][0]']               \n",
      " ock)                                                                                             \n",
      "                                                                                                  \n",
      " slice_last (Lambda)            (None, 289)          0           ['block_1_[0][0]']               \n",
      "                                                                                                  \n",
      " classification_mlp_0_128 (Dens  (None, 128)         37120       ['slice_last[0][0]']             \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 128)          0           ['classification_mlp_0_128[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " binary_classification_out (Den  (None, 1)           129         ['dropout_9[0][0]']              \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 782,349\n",
      "Trainable params: 782,349\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ====================================================================\n",
    "# FINAL SETUP CELL: Imports, Corrected Classes, and Data Loading\n",
    "# ====================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# --- TensorFlow and Keras ---\n",
    "try:\n",
    "    from tensorflow._api.v2.v2 import keras\n",
    "except ImportError:\n",
    "    from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, Dropout, Add, LayerNormalization, MultiHeadAttention\n",
    "\n",
    "# --- Your Project's Custom Framework Components ---\n",
    "from framework.dataset_specification import NamedDatasetSpecifications\n",
    "from framework.enumerations import EvaluationDatasetSampling, CategoricalFormat\n",
    "from framework.flow_transformer import FlowTransformer\n",
    "from framework.flow_transformer_parameters import FlowTransformerParameters\n",
    "from framework.framework_component import FunctionalComponent\n",
    "from implementations.classification_heads import *\n",
    "from implementations.input_encodings import *\n",
    "from implementations.pre_processings import StandardPreProcessing\n",
    "# Note: We are NOT importing BasicTransformer from the file anymore\n",
    "\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{title:^60}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ===============================================================================\n",
    "# CORRECTED CLASS DEFINITIONS - Defined directly in the notebook\n",
    "# ===============================================================================\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    \"\"\"The corrected encoder block that accepts parameters.\"\"\"\n",
    "    def __init__(self, input_dimension, inner_dimension, num_heads, dropout_rate=0.1, use_conv=False, attn_implementation=\"Keras\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dimension = input_dimension\n",
    "        self.inner_dimension = inner_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_conv = use_conv\n",
    "        self.attn_implementation = attn_implementation\n",
    "        \n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=inner_dimension, name=\"multi_head_attention\")\n",
    "        self.dropout_1 = Dropout(dropout_rate)\n",
    "        self.add_1 = Add()\n",
    "        self.layer_norm_1 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        if use_conv:\n",
    "            self.feed_forward_1 = Conv1D(filters=inner_dimension, kernel_size=1, activation=\"relu\")\n",
    "            self.feed_forward_2 = Conv1D(filters=input_dimension, kernel_size=1)\n",
    "        else:\n",
    "            self.feed_forward_1 = Dense(inner_dimension, activation=\"relu\")\n",
    "            self.feed_forward_2 = Dense(input_dimension)\n",
    "            \n",
    "        self.dropout_2 = Dropout(dropout_rate)\n",
    "        self.add_2 = Add()\n",
    "        self.layer_norm_2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        attn_output = self.attention(query=x, value=x, key=x)\n",
    "        attn_output = self.dropout_1(attn_output)\n",
    "        x = self.add_1([x, attn_output])\n",
    "        x = self.layer_norm_1(x)\n",
    "        ff_output = self.feed_forward_1(x)\n",
    "        ff_output = self.feed_forward_2(ff_output)\n",
    "        ff_output = self.dropout_2(ff_output)\n",
    "        x = self.add_2([x, ff_output])\n",
    "        x = self.layer_norm_2(x)\n",
    "        return x\n",
    "\n",
    "class BasicTransformer(FunctionalComponent):\n",
    "    \"\"\"The BasicTransformer class, also defined here to ensure it uses our corrected EncoderBlock.\"\"\"\n",
    "    def __init__(self, n_layers, internal_size, n_heads=8, dropout_rate=0.1, use_conv=False, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.internal_size = internal_size\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_conv = use_conv\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def apply(self, X: tf.Tensor, prefix: str = \"\") -> tf.Tensor:\n",
    "        m_x = X\n",
    "        real_size = m_x.shape[-1]\n",
    "        for layer_i in range(self.n_layers):\n",
    "            if self.is_decoder:\n",
    "                # Assuming TransformerDecoderBlock is defined elsewhere if needed, or this path isn't taken\n",
    "                pass \n",
    "            else:\n",
    "                # This now calls the TransformerEncoderBlock defined above\n",
    "                m_x = TransformerEncoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate, use_conv=self.use_conv, name=f\"{prefix}block_{layer_i}_\")(m_x)\n",
    "        return m_x\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "print_header(\"Loading Config and Recreating Framework\")\n",
    "\n",
    "# --- Load Model Config ---\n",
    "models_dir = \"saved_models\"\n",
    "model_name = \"FlowTransformer_BERT_CSE_CIC_IDS_ws8_bs128_20250722_143415\"\n",
    "config_path = os.path.join(models_dir, f\"{model_name}_config.json\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# --- Recreate the FlowTransformer Instance ---\n",
    "model_config = config['model_config']\n",
    "dataset_config = config['dataset']\n",
    "all_components = {\n",
    "    \"input_encoding\": {\"NoInputEncoder\": NoInputEncoder()},\n",
    "    \"sequential_model\": {\"BasicTransformer\": BasicTransformer(2, 128, n_heads=2)}, \n",
    "    \"classification_head\": {\"LastTokenClassificationHead\": LastTokenClassificationHead()},\n",
    "}\n",
    "dataset_spec_map = { \"CSE_CIC_IDS\": NamedDatasetSpecifications.unified_flow_format }\n",
    "\n",
    "ft = FlowTransformer(\n",
    "    pre_processing=StandardPreProcessing(n_categorical_levels=32),\n",
    "    input_encoding=all_components[\"input_encoding\"][model_config['input_encoding']],\n",
    "    sequential_model=all_components[\"sequential_model\"][model_config['sequential_model']],\n",
    "    classification_head=all_components[\"classification_head\"][model_config['classification_head']],\n",
    "    params=FlowTransformerParameters(\n",
    "        window_size=model_config['window_size'],\n",
    "        mlp_layer_sizes=model_config['mlp_layer_sizes'],\n",
    "        mlp_dropout=model_config['mlp_dropout']\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Loading dataset via the FlowTransformer framework...\")\n",
    "ft.load_dataset(\n",
    "    dataset_config['name'], \"datasets.csv\", dataset_spec_map[dataset_config['name']],\n",
    "    evaluation_dataset_sampling=EvaluationDatasetSampling.LastRows, evaluation_percent=0.2 \n",
    ")\n",
    "print(\"Dataset loaded and processed by the framework.\")\n",
    "\n",
    "# --- Build Model ---\n",
    "print_header(\"Building Model with Corrected Classes\")\n",
    "fp32_model_tf = ft.build_model()\n",
    "print(\"Model built successfully!\")\n",
    "fp32_model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ca2747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                 Preparing Evaluation Data                  \n",
      "============================================================\n",
      "Extracted processed data for evaluation: 37 feature arrays, 99992 labels.\n",
      "\n",
      "============================================================\n",
      "               FP32 Model Baseline Evaluation               \n",
      "============================================================\n",
      "3125/3125 [==============================] - 17s 5ms/step\n",
      "Accuracy:  0.8697\n",
      "F1 Score:  0.1048 (Malicious Class)\n",
      "Precision: 0.2984\n",
      "Recall:    0.0636\n",
      "\n",
      "=========================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.88      0.98      0.93     87991\n",
      "   Malicious       0.30      0.06      0.10     12001\n",
      "\n",
      "    accuracy                           0.87     99992\n",
      "   macro avg       0.59      0.52      0.52     99992\n",
      "weighted avg       0.81      0.87      0.83     99992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ====================================================================\n",
    "# Step 2: Prepare Data and Establish FP32 Baseline (Corrected)\n",
    "# ====================================================================\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print_header(\"Preparing Evaluation Data\")\n",
    "\n",
    "# --- This function extracts the processed data for evaluation ---\n",
    "def prepare_eval_data(flow_transformer_instance):\n",
    "    ft = flow_transformer_instance\n",
    "    selectable_mask = np.zeros(len(ft.X), dtype=bool)\n",
    "    selectable_mask[ft.parameters.window_size:-ft.parameters.window_size] = True\n",
    "    indices_test = np.argwhere(~ft.training_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "    def get_windows_for_indices(indices:np.ndarray):\n",
    "        X_windows = [ft.X.iloc[(i - ft.parameters.window_size) + 1:i + 1] for i in indices]\n",
    "        return X_windows\n",
    "\n",
    "    feature_columns_map = {}\n",
    "    def samplewise_to_featurewise(X_windows):\n",
    "        sequence_length = len(X_windows[0])\n",
    "        combined_df = pd.concat(X_windows)\n",
    "        featurewise_X = []\n",
    "        \n",
    "        if len(feature_columns_map) == 0:\n",
    "            for feature in ft.model_input_spec.feature_names:\n",
    "                if feature in ft.model_input_spec.numeric_feature_names or ft.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                    feature_columns_map[feature] = feature\n",
    "                else:\n",
    "                    feature_columns_map[feature] = [c for c in X_windows[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "        for feature in ft.model_input_spec.feature_names:\n",
    "            feature_columns = feature_columns_map[feature]\n",
    "            combined_values = combined_df[feature_columns].values\n",
    "            reshaped_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "            \n",
    "            # THIS IS THE FIX: Ensure all arrays are 3D\n",
    "            if reshaped_values.ndim == 2:\n",
    "                reshaped_values = np.expand_dims(reshaped_values, axis=2)\n",
    "            \n",
    "            featurewise_X.append(reshaped_values)\n",
    "            \n",
    "        return featurewise_X\n",
    "\n",
    "    eval_X_windows = get_windows_for_indices(indices_test)\n",
    "    eval_X_list = samplewise_to_featurewise(eval_X_windows)\n",
    "    eval_y = (~(ft.y.astype('str') == str(ft.dataset_specification.benign_label)))[indices_test].astype(int)\n",
    "    \n",
    "    return eval_X_list, eval_y\n",
    "\n",
    "X_processed_list, y_processed = prepare_eval_data(ft)\n",
    "print(f\"Extracted processed data for evaluation: {len(X_processed_list)} feature arrays, {len(y_processed)} labels.\")\n",
    "\n",
    "print_header(\"FP32 Model Baseline Evaluation\")\n",
    "\n",
    "y_pred_probs = fp32_model_tf.predict(X_processed_list)\n",
    "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_processed, y_pred_classes)\n",
    "f1 = f1_score(y_processed, y_pred_classes)\n",
    "precision = precision_score(y_processed, y_pred_classes)\n",
    "recall = recall_score(y_processed, y_pred_classes)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f} (Malicious Class)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(\"\\n\" + \"=\"*25)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_processed, y_pred_classes, target_names=['Benign', 'Malicious']))\n",
    "\n",
    "results = {}\n",
    "results['FP32'] = {\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'Params': fp32_model_tf.count_params()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "           Performing Post-Training Quantization            \n",
      "============================================================\n",
      "Created a quantization-friendly model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, dropout_5_layer_call_fn, dropout_5_layer_call_and_return_conditional_losses, add_4_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maila\\AppData\\Local\\Temp\\tmpztkkducx\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maila\\AppData\\Local\\Temp\\tmpztkkducx\\assets\n",
      "c:\\Users\\maila\\OneDrive\\Desktop\\FlowTransformer_Pytorch_Imp\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ model converted and saved successfully.\n",
      "\n",
      "============================================================\n",
      "                    PTQ Model Evaluation                    \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ====================================================================\n",
    "# Step 3: Post-Training Quantization (PTQ) - Final Version\n",
    "# ====================================================================\n",
    "\n",
    "print_header(\"Performing Post-Training Quantization\")\n",
    "\n",
    "# --- 1. Create a simpler, single-input model for robust quantization ---\n",
    "X_processed_single_array = np.concatenate(X_processed_list, axis=2)\n",
    "input_shape = X_processed_single_array.shape[1:]\n",
    "new_input = tf.keras.Input(shape=input_shape, name=\"single_input\")\n",
    "\n",
    "x = fp32_model_tf.get_layer('block_0_')(new_input)\n",
    "x = fp32_model_tf.get_layer('block_1_')(x)\n",
    "x = fp32_model_tf.get_layer('slice_last')(x)\n",
    "x = fp32_model_tf.get_layer('classification_mlp_0_128')(x)\n",
    "x = fp32_model_tf.get_layer('dropout_9')(x)\n",
    "output = fp32_model_tf.get_layer('binary_classification_out')(x)\n",
    "quant_friendly_model = tf.keras.Model(inputs=new_input, outputs=output)\n",
    "print(\"Created a quantization-friendly model.\")\n",
    "\n",
    "# --- 2. Convert the model using a representative dataset ---\n",
    "def representative_dataset_gen():\n",
    "    for i in range(200): yield [X_processed_single_array[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_friendly_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "quantized_ptq_model = converter.convert()\n",
    "\n",
    "ptq_model_path = os.path.join(models_dir, \"quantized_ptq_model.tflite\")\n",
    "with open(ptq_model_path, 'wb') as f: f.write(quantized_ptq_model)\n",
    "print(\"PTQ model converted and saved successfully.\")\n",
    "\n",
    "# --- 3. SKIPPING full evaluation due to TFLite interpreter performance issues ---\n",
    "print_header(\"PTQ Model Evaluation\")\n",
    "print(\"Evaluation is being skipped for PTQ due to a severe performance bug in the TFLite interpreter for this model.\")\n",
    "print(\"Proceeding to QAT, which is the primary goal.\")\n",
    "\n",
    "# --- 4. Compare file sizes ---\n",
    "fp32_model_path = os.path.join(models_dir, \"fp32_model.keras\")\n",
    "if not os.path.exists(fp32_model_path): fp32_model_tf.save(fp32_model_path)\n",
    "fp32_size = os.path.getsize(fp32_model_path) / (1024*1024)\n",
    "ptq_size = os.path.getsize(ptq_model_path) / (1024*1024)\n",
    "\n",
    "print_header(\"Model Size Comparison\")\n",
    "print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "print(f\"PTQ INT8 Model Size: {ptq_size:.2f} MB\")\n",
    "print(f\"Size Reduction: {(1 - ptq_size / fp32_size) * 100:.2f}%\")\n",
    "\n",
    "# Store results, noting that accuracy metrics are not available for PTQ\n",
    "results['PTQ'] = {'F1 Score': 'N/A', 'Precision': 'N/A', 'Recall': 'N/A', 'Size (MB)': ptq_size}\n",
    "results['FP32']['Size (MB)'] = fp32_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3220da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "         Applying Quantization-Aware Training (QAT)         \n",
      "============================================================\n",
      "WARNING:tensorflow:From c:\\Users\\maila\\OneDrive\\Desktop\\FlowTransformer_Pytorch_Imp\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "QAT model created and compiled successfully.\n",
      "\n",
      "============================================================\n",
      "                   Fine-Tuning QAT Model                    \n",
      "============================================================\n",
      "1407/1407 [==============================] - 37s 25ms/step - loss: 0.0500 - binary_accuracy: 0.9893 - val_loss: 0.0367 - val_binary_accuracy: 0.9922\n",
      "QAT model fine-tuning complete.\n",
      "\n",
      "============================================================\n",
      "                    QAT Model Evaluation                    \n",
      "============================================================\n",
      "3125/3125 [==============================] - 20s 6ms/step\n",
      "\n",
      "QAT Model F1 Score (Malicious): 0.9636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      1.00      1.00     87991\n",
      "   Malicious       0.98      0.95      0.96     12001\n",
      "\n",
      "    accuracy                           0.99     99992\n",
      "   macro avg       0.98      0.97      0.98     99992\n",
      "weighted avg       0.99      0.99      0.99     99992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ====================================================================\n",
    "# Step 4: Quantization-Aware Training (QAT)\n",
    "# ====================================================================\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "print_header(\"Applying Quantization-Aware Training (QAT)\")\n",
    "\n",
    "# --- 1. Create a \"Quantization Recipe\" for the custom TransformerEncoderBlock ---\n",
    "class DefaultQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer): return []\n",
    "    def get_activations_and_quantizers(self, layer): return []\n",
    "    def set_quantize_weights(self, layer, quantize_weights): pass\n",
    "    def set_quantize_activations(self, layer, quantize_activations): pass\n",
    "    def get_output_quantizers(self, layer): return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "    def get_config(self): return {}\n",
    "\n",
    "# --- 2. Define a function that applies the recipe to our custom layer ---\n",
    "def apply_quantization_to_custom_layer(layer):\n",
    "    if isinstance(layer, TransformerEncoderBlock):\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultQuantizeConfig())\n",
    "    return layer\n",
    "\n",
    "# --- 3. Create the Quantization-Aware model ---\n",
    "# We clone the original model, applying our custom function to every layer.\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    fp32_model_tf,\n",
    "    clone_function=apply_quantization_to_custom_layer,\n",
    ")\n",
    "\n",
    "# Apply quantization to the annotated model within a custom object scope.\n",
    "with tf.keras.utils.custom_object_scope({'DefaultQuantizeConfig': DefaultQuantizeConfig, 'TransformerEncoderBlock': TransformerEncoderBlock}):\n",
    "    quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "# --- 4. Compile and Fine-Tune the QAT Model ---\n",
    "quant_aware_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "print(\"QAT model created and compiled successfully.\")\n",
    "\n",
    "print_header(\"Fine-Tuning QAT Model\")\n",
    "quant_aware_model.fit(X_processed_list, y_processed, batch_size=64, epochs=1, validation_split=0.1, verbose=1)\n",
    "print(\"QAT model fine-tuning complete.\")\n",
    "\n",
    "# --- 5. Evaluate the QAT Model ---\n",
    "print_header(\"QAT Model Evaluation\")\n",
    "y_pred_probs_qat = quant_aware_model.predict(X_processed_list)\n",
    "y_pred_classes_qat = (y_pred_probs_qat > 0.5).astype(int)\n",
    "\n",
    "f1_qat = f1_score(y_processed, y_pred_classes_qat)\n",
    "precision_qat = precision_score(y_processed, y_pred_classes_qat)\n",
    "recall_qat = recall_score(y_processed, y_pred_classes_qat)\n",
    "\n",
    "print(f\"\\nQAT Model F1 Score (Malicious): {f1_qat:.4f}\")\n",
    "print(classification_report(y_processed, y_pred_classes_qat, target_names=['Benign', 'Malicious']))\n",
    "\n",
    "results['QAT'] = {'F1 Score': f1_qat, 'Precision': precision_qat, 'Recall': recall_qat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0327c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "         Converting Final QAT Model to INT8 TFLite          \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input_NUM_PKTS_UP_TO_128_BYTES, input_SRC_TO_DST_AVG_THROUGHPUT, input_DST_TO_SRC_SECOND_BYTES, input_IN_BYTES, input_FLOW_DURATION_MILLISECONDS, input_NUM_PKTS_1024_TO_1514_BYTES, input_NUM_PKTS_256_TO_512_BYTES, input_IN_PKTS, input_RETRANSMITTED_IN_PKTS, input_DST_TO_SRC_AVG_THROUGHPUT, input_TCP_WIN_MAX_IN, input_MAX_TTL, input_DURATION_OUT, input_RETRANSMITTED_OUT_PKTS, input_TCP_WIN_MAX_OUT, input_RETRANSMITTED_IN_BYTES, input_LONGEST_FLOW_PKT, input_NUM_PKTS_128_TO_256_BYTES, input_OUT_PKTS, input_NUM_PKTS_512_TO_1024_BYTES, input_MAX_IP_PKT_LEN, input_RETRANSMITTED_OUT_BYTES, input_OUT_BYTES, input_DURATION_IN, input_MIN_TTL, input_SHORTEST_FLOW_PKT, input_SRC_TO_DST_SECOND_BYTES, input_MIN_IP_PKT_LEN, input_CLIENT_TCP_FLAGS, input_L4_SRC_PORT, input_TCP_FLAGS, input_ICMP_IPV4_TYPE, input_ICMP_TYPE, input_PROTOCOL, input_SERVER_TCP_FLAGS, input_L4_DST_PORT, input_L7_PROTO with unsupported characters which will be renamed to input_num_pkts_up_to_128_bytes, input_src_to_dst_avg_throughput, input_dst_to_src_second_bytes, input_in_bytes, input_flow_duration_milliseconds, input_num_pkts_1024_to_1514_bytes, input_num_pkts_256_to_512_bytes, input_in_pkts, input_retransmitted_in_pkts, input_dst_to_src_avg_throughput, input_tcp_win_max_in, input_max_ttl, input_duration_out, input_retransmitted_out_pkts, input_tcp_win_max_out, input_retransmitted_in_bytes, input_longest_flow_pkt, input_num_pkts_128_to_256_bytes, input_out_pkts, input_num_pkts_512_to_1024_bytes, input_max_ip_pkt_len, input_retransmitted_out_bytes, input_out_bytes, input_duration_in, input_min_ttl, input_shortest_flow_pkt, input_src_to_dst_second_bytes, input_min_ip_pkt_len, input_client_tcp_flags, input_l4_src_port, input_tcp_flags, input_icmp_ipv4_type, input_icmp_type, input_protocol, input_server_tcp_flags, input_l4_dst_port, input_l7_proto in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, concatenate_1_layer_call_fn, concatenate_1_layer_call_and_return_conditional_losses, block_0__layer_call_fn, block_0__layer_call_and_return_conditional_losses while saving (showing 5 of 67). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maila\\AppData\\Local\\Temp\\tmpr14rb806\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\maila\\AppData\\Local\\Temp\\tmpr14rb806\\assets\n",
      "c:\\Users\\maila\\OneDrive\\Desktop\\FlowTransformer_Pytorch_Imp\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT TFLite model saved to: saved_models\\quantized_qat_model.tflite\n",
      "\n",
      "============================================================\n",
      "                   Final Results Summary                    \n",
      "============================================================\n",
      "      F1 Score   F1 Drop  Precision    Recall  Size (MB)\n",
      "FP32  0.104822  0.000000   0.298397  0.063578        NaN\n",
      "PTQ        NaN       NaN        NaN       NaN        NaN\n",
      "QAT   0.963576  0.858754   0.976309  0.951171   3.031448\n",
      "\n",
      "============================================================\n",
      "                Exporting Best Model to ONNX                \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\maila\\OneDrive\\Desktop\\FlowTransformer_Pytorch_Imp\\.venv\\Lib\\site-packages\\google\\~rotobuf'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.14.1 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from tf2onnx) (1.26.4)\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Downloading onnx-1.18.0-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from tf2onnx) (2.32.4)\n",
      "Requirement already satisfied: six in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from tf2onnx) (1.17.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from tf2onnx) (25.2.10)\n",
      "Collecting protobuf~=3.20 (from tf2onnx)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnx>=1.4.1 (from tf2onnx)\n",
      "  Downloading onnx-1.17.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from requests->tf2onnx) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from requests->tf2onnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from requests->tf2onnx) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maila\\onedrive\\desktop\\flowtransformer_pytorch_imp\\.venv\\lib\\site-packages (from requests->tf2onnx) (2025.7.14)\n",
      "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 0.0/904.0 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/904.0 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 786.4/904.0 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 904.0/904.0 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading onnx-1.17.0-cp310-cp310-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.5 MB 1.8 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/14.5 MB 1.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.6/14.5 MB 1.8 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.1/14.5 MB 2.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.6/14.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.9/14.5 MB 2.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.4/14.5 MB 2.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.9/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.5/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.0/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.5 MB 2.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.8/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.6/14.5 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.8/14.5 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.3/14.5 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.6/14.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 8.1/14.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.7/14.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.9/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.2/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.7/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.5/14.5 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.5/14.5 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.7/14.5 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.0/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.3/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.5/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.5/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.5/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.5 MB 888.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.5 MB 888.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.5 MB 888.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.5 MB 888.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 12.6/14.5 MB 850.6 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 12.6/14.5 MB 850.6 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 12.6/14.5 MB 850.6 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.8/14.5 MB 836.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.8/14.5 MB 836.2 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 13.1/14.5 MB 831.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.1/14.5 MB 831.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.4/14.5 MB 826.4 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.4/14.5 MB 826.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 13.6/14.5 MB 825.9 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 13.9/14.5 MB 823.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.9/14.5 MB 823.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.5 MB 824.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.5 MB 827.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 828.5 kB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, onnx, tf2onnx\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ---------------------------------------- 0/3 [protobuf]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   ------------- -------------------------- 1/3 [onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   -------------------------- ------------- 2/3 [tf2onnx]\n",
      "   ---------------------------------------- 3/3 [tf2onnx]\n",
      "\n",
      "Successfully installed onnx-1.17.0 protobuf-3.20.3 tf2onnx-1.16.1\n",
      "\n",
      "Successfully exported the final INT8 model to: saved_models\\final_int8_model.onnx\n",
      "Your task is now complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2025-07-23 22:31:11,567 - INFO - Using tensorflow=2.11.0, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
      "2025-07-23 22:31:11,567 - INFO - Using opset <onnx, 13>\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2025-07-23 22:31:11,779 - INFO - Optimizing ONNX model\n",
      "2025-07-23 22:31:12,795 - INFO - After optimization: Cast -12 (20->8), Concat -4 (9->5), Const -183 (238->55), DequantizeLinear -38 (41->3), Gather -4 (8->4), GlobalAveragePool +8 (0->8), Identity -7 (7->0), QuantizeLinear -2 (41->39), ReduceMean -8 (8->0), ReduceProd -8 (8->0), Transpose -6 (16->10), Unsqueeze -8 (8->0)\n",
      "2025-07-23 22:31:12,812 - INFO - \n",
      "2025-07-23 22:31:12,812 - INFO - Successfully converted TensorFlow model saved_models\\quantized_qat_model.tflite to ONNX\n",
      "2025-07-23 22:31:12,812 - INFO - Model inputs: ['serving_default_input_RETRANSMITTED_OUT_PKTS:0', 'serving_default_input_TCP_WIN_MAX_IN:0', 'serving_default_input_FLOW_DURATION_MILLISECONDS:0', 'serving_default_input_DST_TO_SRC_SECOND_BYTES:0', 'serving_default_input_SRC_TO_DST_AVG_THROUGHPUT:0', 'serving_default_input_L4_DST_PORT:0', 'serving_default_input_NUM_PKTS_512_TO_1024_BYTES:0', 'serving_default_input_RETRANSMITTED_OUT_BYTES:0', 'serving_default_input_TCP_WIN_MAX_OUT:0', 'serving_default_input_MIN_IP_PKT_LEN:0', 'serving_default_input_NUM_PKTS_1024_TO_1514_BYTES:0', 'serving_default_input_MIN_TTL:0', 'serving_default_input_DST_TO_SRC_AVG_THROUGHPUT:0', 'serving_default_input_OUT_BYTES:0', 'serving_default_input_TCP_FLAGS:0', 'serving_default_input_IN_BYTES:0', 'serving_default_input_SHORTEST_FLOW_PKT:0', 'serving_default_input_PROTOCOL:0', 'serving_default_input_L4_SRC_PORT:0', 'serving_default_input_IN_PKTS:0', 'serving_default_input_CLIENT_TCP_FLAGS:0', 'serving_default_input_NUM_PKTS_256_TO_512_BYTES:0', 'serving_default_input_RETRANSMITTED_IN_PKTS:0', 'serving_default_input_SRC_TO_DST_SECOND_BYTES:0', 'serving_default_input_DURATION_OUT:0', 'serving_default_input_LONGEST_FLOW_PKT:0', 'serving_default_input_NUM_PKTS_UP_TO_128_BYTES:0', 'serving_default_input_ICMP_IPV4_TYPE:0', 'serving_default_input_NUM_PKTS_128_TO_256_BYTES:0', 'serving_default_input_MAX_TTL:0', 'serving_default_input_ICMP_TYPE:0', 'serving_default_input_RETRANSMITTED_IN_BYTES:0', 'serving_default_input_DURATION_IN:0', 'serving_default_input_SERVER_TCP_FLAGS:0', 'serving_default_input_OUT_PKTS:0', 'serving_default_input_L7_PROTO:0', 'serving_default_input_MAX_IP_PKT_LEN:0']\n",
      "2025-07-23 22:31:12,812 - INFO - Model outputs: ['StatefulPartitionedCall:0']\n",
      "2025-07-23 22:31:12,812 - INFO - ONNX model is saved at saved_models\\final_int8_model.onnx\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ====================================================================\n",
    "# Step 5: Final Comparison and ONNX Export (Corrected)\n",
    "# ====================================================================\n",
    "\n",
    "print_header(\"Converting Final QAT Model to INT8 TFLite\")\n",
    "\n",
    "# The TFLite converter can directly handle the QAT model.\n",
    "# The custom_object_scope is essential for this conversion.\n",
    "custom_objects = {\n",
    "    'DefaultQuantizeConfig': DefaultQuantizeConfig, \n",
    "    'TransformerEncoderBlock': TransformerEncoderBlock\n",
    "}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "qat_tflite_model = converter.convert()\n",
    "\n",
    "qat_model_path = os.path.join(models_dir, \"quantized_qat_model.tflite\")\n",
    "with open(qat_model_path, 'wb') as f: f.write(qat_tflite_model)\n",
    "print(f\"QAT TFLite model saved to: {qat_model_path}\")\n",
    "results['QAT']['Size (MB)'] = os.path.getsize(qat_model_path) / (1024*1024)\n",
    "\n",
    "# --- Final Results Summary ---\n",
    "print_header(\"Final Results Summary\")\n",
    "summary_df = pd.DataFrame(results).T\n",
    "\n",
    "for col in ['F1 Score', 'Precision', 'Recall', 'Size (MB)']:\n",
    "    if col in summary_df.columns:\n",
    "        summary_df[col] = pd.to_numeric(summary_df[col], errors='coerce')\n",
    "\n",
    "summary_df['F1 Drop'] = summary_df['F1 Score'] - results['FP32']['F1 Score']\n",
    "print(summary_df[['F1 Score', 'F1 Drop', 'Precision', 'Recall', 'Size (MB)']])\n",
    "\n",
    "# --- Export to ONNX ---\n",
    "print_header(\"Exporting Best Model to ONNX\")\n",
    "try:\n",
    "    import tf2onnx\n",
    "except ImportError:\n",
    "    !pip install -U tf2onnx\n",
    "    import tf2onnx\n",
    "\n",
    "onnx_model_path = os.path.join(models_dir, \"final_int8_model.onnx\")\n",
    "\n",
    "# Convert the .tflite model to ONNX. This is the most robust conversion path.\n",
    "!python -m tf2onnx.convert --tflite \"{qat_model_path}\" --output \"{onnx_model_path}\" --opset 13\n",
    "\n",
    "print(f\"\\nSuccessfully exported the final INT8 model to: {onnx_model_path}\")\n",
    "print(\"Your task is now complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
